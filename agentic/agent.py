"""
Agentic Retrieval using a custom LangGraph StateGraph.

Architecture upgrade from create_react_agent â†’ explicit node graph:

    START â†’ [PLANNER] â†’ [EXECUTOR] â‡„ [TOOLS] â†’ [SYNTHESIZER] â†’ END

Nodes:
  PLANNER    â€” cheap mini-model generates a step-by-step retrieval plan before
               any tools are called. Plan is stored in state and shown to user.
  EXECUTOR   â€” gpt-4o with bound tools decides the next action (tool call or done).
               Receives the plan as a system-level instruction on its first call.
  TOOLS      â€” ToolNode executes whatever tool the executor selected.
  SYNTHESIZER â€” structured final answer generation using Pydantic guardrails.

Why this matters over create_react_agent:
  - Explicit planning step makes reasoning auditable and transparent.
  - Planner runs on gpt-4o-mini (cheap) before the expensive execution loop.
  - Graph structure allows conditional routing and easy extension (e.g. reflection nodes).
  - Plan is surfaced to the user so they can see the agent's strategy.
"""

from __future__ import annotations

import time
from dataclasses import dataclass, field
from typing import Annotated, Literal, TypedDict

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode

from agentic.tools import ALL_TOOLS
from langchain_community.callbacks import get_openai_callback
from shared.metrics import estimate_cost
from shared.guardrails import AgenticAnswer


# â”€â”€ Graph State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class AgentGraphState(TypedDict):
    """State passed between nodes in the LangGraph execution graph."""
    messages: Annotated[list, add_messages]
    plan: str          # Generated by PLANNER before any tool calls
    iteration: int     # Counts EXECUTORâ†’TOOLS cycles


# â”€â”€ Prompts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PLANNER_PROMPT = """\
You are a research planner. Given a question about a GitHub portfolio, \
produce a numbered step-by-step retrieval plan (2-4 steps).

Available tools:
  â€¢ search_knowledge_base â€” semantic search across all READMEs
  â€¢ get_full_document     â€” fetch the complete README for a specific repo
  â€¢ list_repos            â€” list all repos in the knowledge base
  â€¢ get_github_stats      â€” live GitHub data (stars, issues, activity)

Question: {question}

Output only the numbered plan. Be specific about which tool each step uses.\
"""

EXECUTOR_SYSTEM = """\
You are executing a retrieval plan to answer questions about a developer's GitHub portfolio.

Plan to follow:
{plan}

Use the tools systematically to gather information. Adapt the plan if intermediate \
results reveal a better path. When you have enough information, produce a final answer \
without calling any more tools.\
"""

SYNTHESIZER_SYSTEM = """\
You just completed a multi-step research task about a GitHub portfolio. \
Based on the conversation history, produce a final structured answer.\
"""


# â”€â”€ Result dataclass â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class AgenticResult:
    """Result from the agentic pipeline, including trace info for comparison."""
    question: str
    answer: str
    plan: str = ""                         # LangGraph planner output
    llm_calls: int = 0
    tool_calls: list[dict] = field(default_factory=list)
    total_tokens: int = 0
    prompt_tokens: int = 0
    completion_tokens: int = 0
    cost_usd: float = 0.0
    latency_seconds: float = 0.0
    steps: list[str] = field(default_factory=list)
    guardrails: AgenticAnswer | None = None


# â”€â”€ Graph Builder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _build_graph(llm_executor: ChatOpenAI, llm_synthesizer: ChatOpenAI):
    """Compile and return the LangGraph StateGraph."""

    llm_with_tools = llm_executor.bind_tools(ALL_TOOLS)
    llm_structured  = llm_synthesizer.with_structured_output(AgenticAnswer)
    tool_node        = ToolNode(ALL_TOOLS)

    # â”€â”€ Node: PLANNER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def planner(state: AgentGraphState) -> dict:
        """Generate a retrieval plan using a cheap model before any tools fire."""
        question = ""
        for msg in reversed(state["messages"]):
            if isinstance(msg, HumanMessage):
                question = msg.content
                break
        planner_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        resp = planner_llm.invoke(PLANNER_PROMPT.format(question=question))
        return {"plan": resp.content, "iteration": 0}

    # â”€â”€ Node: EXECUTOR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def executor(state: AgentGraphState) -> dict:
        """LLM-with-tools decides the next action; injects plan on first call."""
        messages = list(state["messages"])
        if state.get("iteration", 0) == 0 and state.get("plan"):
            sys_msg = SystemMessage(content=EXECUTOR_SYSTEM.format(plan=state["plan"]))
            messages = [sys_msg] + messages
        resp = llm_with_tools.invoke(messages)
        return {"messages": [resp], "iteration": state.get("iteration", 0) + 1}

    # â”€â”€ Node: SYNTHESIZER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def synthesizer(state: AgentGraphState) -> dict:
        """Generate a validated, structured final answer using Pydantic guardrails."""
        try:
            resp: AgenticAnswer = llm_structured.invoke(
                [SystemMessage(content=SYNTHESIZER_SYSTEM)] + list(state["messages"])
            )
            return {"messages": [AIMessage(content=resp.answer, additional_kwargs={"guardrails": resp.model_dump()})]}
        except Exception:
            # Guardrails failed â€” final executor AIMessage already has the answer
            return {}

    # â”€â”€ Routing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def route_after_executor(state: AgentGraphState) -> Literal["tools", "synthesizer"]:
        last = state["messages"][-1]
        if isinstance(last, AIMessage) and getattr(last, "tool_calls", None):
            return "tools"
        return "synthesizer"

    # â”€â”€ Assemble Graph â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    graph = StateGraph(AgentGraphState)
    graph.add_node("planner",     planner)
    graph.add_node("executor",    executor)
    graph.add_node("tools",       tool_node)
    graph.add_node("synthesizer", synthesizer)

    graph.add_edge(START,        "planner")
    graph.add_edge("planner",    "executor")
    graph.add_conditional_edges(
        "executor",
        route_after_executor,
        {"tools": "tools", "synthesizer": "synthesizer"},
    )
    graph.add_edge("tools",      "executor")
    graph.add_edge("synthesizer", END)

    return graph.compile()


# â”€â”€ Public API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_agentic_pipeline(
    question: str,
    model: str = "gpt-4o",
    max_iterations: int = 8,
    verbose: bool = True,
) -> AgenticResult:
    """
    Execute the LangGraph agentic retrieval pipeline.

    Args:
        question: User's question.
        model: OpenAI model for executor + synthesizer.
        max_iterations: Max EXECUTORâ†’TOOLS loops before the graph is force-stopped.
        verbose: Unused (kept for API compatibility).

    Returns:
        AgenticResult with answer, plan, tool call trace, and performance metrics.
    """
    start = time.time()

    llm_executor    = ChatOpenAI(model=model, temperature=0)
    llm_synthesizer = ChatOpenAI(model=model, temperature=0)
    graph = _build_graph(llm_executor, llm_synthesizer)

    with get_openai_callback() as cb:
        result = graph.invoke(
            {"messages": [HumanMessage(content=question)], "plan": "", "iteration": 0},
            config={"recursion_limit": max_iterations * 2 + 6},
        )

    prompt_tokens     = cb.prompt_tokens
    completion_tokens = cb.completion_tokens
    total_tokens      = cb.total_tokens
    cost_usd          = estimate_cost(model, prompt_tokens, completion_tokens)

    # â”€â”€ Parse messages into trace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    messages   = result.get("messages", [])
    plan       = result.get("plan", "")
    steps: list[str]       = []
    tool_calls: list[dict] = []
    llm_calls  = 0
    answer     = "No answer produced."
    guardrails: AgenticAnswer | None = None

    if plan:
        steps.append(f"ðŸ“‹ PLAN:\n{plan}")

    for msg in messages:
        if isinstance(msg, AIMessage):
            llm_calls += 1
            if msg.tool_calls:
                for tc in msg.tool_calls:
                    steps.append(f"THOUGHT: Decided to use '{tc['name']}'")
                    steps.append(f"ACTION: {tc['name']}({tc['args']})")
            else:
                answer = msg.content if isinstance(msg.content, str) else str(msg.content)
                steps.append(f"FINAL ANSWER: Generated after {len(tool_calls)} tool calls")
                # Check if guardrails data attached by synthesizer
                gdata = (msg.additional_kwargs or {}).get("guardrails")
                if gdata:
                    try:
                        guardrails = AgenticAnswer(**gdata)
                    except Exception:
                        pass

        elif isinstance(msg, ToolMessage):
            full_output = str(msg.content)
            steps.append(f"OBSERVATION: {full_output[:200]}...")
            tool_calls.append({
                "tool":            msg.name,
                "input":           "",
                "output_preview":  full_output[:200],
                "output_full":     full_output,
            })

    elapsed = time.time() - start

    return AgenticResult(
        question=question,
        answer=answer,
        plan=plan,
        llm_calls=llm_calls,
        tool_calls=tool_calls,
        total_tokens=total_tokens,
        prompt_tokens=prompt_tokens,
        completion_tokens=completion_tokens,
        cost_usd=cost_usd,
        latency_seconds=round(elapsed, 2),
        steps=steps,
        guardrails=guardrails,
    )
